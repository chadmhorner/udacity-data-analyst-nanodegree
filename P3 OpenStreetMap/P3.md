
# An Analysis of New York City OpenStreetMap Data

In this report, we will examine the OpenStreetMap data for New York, NY. I obtained this custom subset of data using [Mapzen](https://mapzen.com/data/metro-extracts/). Within New York, I focused my examination on the borough of Manhattan specifically. As a result of the manner in which the map extract is obtained, there is some data from the surrounding boroughs and states included in the dataset. 

My first step was to iterate through the nodes and ways, discovering what type of data they contained in the tags. I settled on a set of tags that would be interesting to examine, including the 'addr' tags, 'amenity', 'cuisine', and 'religion'. After auditing the data, I noticed the following issues that I would want to correct before reading it into the database:

* Street types are incorrect and/or not uniform (e.g. some are 'Street' where others are 'St.')
* Prefixes to street addresses also are not uniform (e.g. some are 'East' where others are 'E.')
* Zipcodes were not all valid 
* City names were not uniform
* Cuisines were not in a uniform format 

We'll examine each of these issues, and the methods for correcting them, in turn.

## Street Types
To correct the street types, I iterated through all of the nodes and ways tags, grabbing the values for any tags with k = 'addr:street'. Using the street name regular expression that we used in the lessons before the project, I extracted the street type and checked if it was in a list of expected streets (assembling this list was an iterative process). If it was not, I either: 
1. Added the street type to the expected street list, if it was a valid street type
2. Mapped it to an expected street type via a street mapping dictionary, if it was a valid street type in a different form (e.g. 'Blvd' instead of 'Boulevard')
3. Took no action, in which case the street would eventually be excluded from the database


## Street Prefixes
This process is straightforward - we want uniform street names in our data, so we don't want to have 'East 34th Street', 'E. 34th Street', and 'E 34th Street' - they're all the same thing! To fix this, I created a simple mapping dictionary that would take in a street name and adjust the prefix if necessary.

## Zipcodes
There were a few different types of invalid zipcodes that I came across in the data:
* including the state before the code, e.g. 'NJ 07036'
* including the extended version of the code, e.g. '08901-1340'
* including extra whitespace, e.g. ' 10010'
* fewer than 5 characters, e.g. '1341'

The first three cases can be analyzed and have the intended postal code extracted; in the case of the fourth, the tag was simply skipped. 

## City Names
I wanted all of the city names to have a uniform format, so the fixer function below ensures a uniform capitalization scheme. It also adjusts the common occurrence 'New York City' to simply 'New York', to ensure uniformity there. There were also some city names that included the state alongside it - e.g. 'New York, NY' or 'Queens NY'. This function adjusts for those occurrences as well. 

## Cuisines
I consider myself a foodie, and thus I knew going in that a significant portion of my analysis would be based off of the restaurant data in the dataset. I audited the data to create a list of expected cuisines, as well as a small mapping dictionary to account for cases where there are multiple valid spellings (e.g. 'doughnut' or 'donut'), as well as cases where I wanted to incorporate various cuisines under a common header (e.g. 'tacos' or 'taco' should simply be counted along with all other 'mexican' food). 

Many of the cuisine tags had multipe values listed, e.g. 'american;french;steak'. In these cases I simply used the first valid cuisine in the list, under the presumption that the signature attribute would be listed first. For 'fusion' cuisines that incorporated a hyphen, e.g. 'korean-american', I simply used the leading cuisine, again assuming that 'korean-american' is really more 'korean' than 'american'. 

All of the cuisines were converted to lower case in the interest of uniformity. 

## Adding Neighborhood Data
As part of my analysis, I wanted to enhance the data by classifying all nodes based on the neighborhoods they are in. I thought this would allow for some more interesting analyses later on. 

To do this, I read up on the [python shapefile library](https://pypi.python.org/pypi/pyshp). After doing some digging around, I had found that [Zillow](http://www.zillow.com/howto/api/neighborhood-boundaries.htm) provides neighborhood boundaries in the form of shapefile objects. Using this data along with a function that tests if a set of coordinates falls inside a shape (credit to the [GIS stackexchange site](http://gis.stackexchange.com/questions/121469/get-shape-file-polygon-attribute-value-at-a-specific-point-using-python-e-g-vi)), I was able to assign all nodes a neighborhood attribute (as I was only testing for the neighborhoods in Manhattan, many nodes ended up without a value for their neighborhood attribute). This required a small modification to be made to the schema.py file, detailed below:

```python
schema = {
    'node': {
        'type': 'dict',
        'schema': {
            'id': {'required': True, 'type': 'integer', 'coerce': int},
            'lat': {'required': True, 'type': 'float', 'coerce': float},
            'lon': {'required': True, 'type': 'float', 'coerce': float},
            'user': {'required': True, 'type': 'string'},
            'uid': {'required': True, 'type': 'integer', 'coerce': int},
            'version': {'required': True, 'type': 'string'},
            'changeset': {'required': True, 'type': 'integer', 'coerce': int},
            'timestamp': {'required': True, 'type': 'string'},
            'neighborhood': {'required': False, 'type': 'string'}
        }
    }
#other types below...
}
```

While the neighborhood boundaries are a bit out of date (they don't include "newer" neighborhoods like Nolita and Hell's Kitchen, using older nomenclature for some of the neighborhoods - like Clinton for Hell's Kitchen - or less specific boundaries - Gramercy includes parts of Gramercy, Union Square, Flatiron, and Kips Bay), they still provide useful information! 

## An Overview of the Dataset
Here I provide some key statistics about the data we're examining. 

### File sizes
* newyork.osm: 262.5 MB
* my_db.db: 395.2 MB
* nodes.csv: 98 MB
* nodes_tags.csv: 7.4 MB
* ways.csv: 10.7 MB
* ways_tags.csv: 29.7 MB
* ways_nodes.csv: 33.6 MB

### Counts of data
* number of nodes: 1000051
* number of ways: 160518
* number of unique users: 1730

```SQL
select count(*) from nodes;
select count(*) from ways;
select count(distinct(nodesways.uid))
from (select uid from nodes union all select uid from ways) nodesways;
```

## In-depth Exploration of Data
### Restaurants
Here I utilize the restaurant/cuisine data, along with the neighborhood info, to answer more interesting questions. 

#### How many restaurants of each cuisine are there? 

```SQL
select nodes_tags.value, count(*) as count
from nodes_tags
where nodes_tags.key = 'cuisine'
group by nodes_tags.value
order by count desc
limit 10;
```

`
"coffee shop",183
pizza,123
italian,122
burger,99
mexican,94
american,87
japanese,63
chinese,62
sandwich,53
indian,47
`

#### How many restaurants does each neighborhood have?

```SQL
select nodes.neighborhood, count(*) as count
from nodes, nodes_tags
where nodes.id = nodes_tags.id
and nodes_tags.key = 'cuisine'
and nodes.neighborhood != ''
group by nodes.neighborhood
order by count desc;
```

`
"Upper West Side",165
Midtown,150
Chelsea,115
Gramercy,109
"East Village",70
"Financial District",62
"Greenwich Village",59
"Upper East Side",50
Clinton,42
"Lower East Side",41
"Murray Hill",28
Tribeca,28
"Garment District",26
"West Village",25
Soho,23
Harlem,22
"Washington Heights",18
"Morningside Heights",16
"Hamilton Heights",8
Chinatown,7
"Battery Park",5
"Little Italy",5
Yorkville,5
"Carnegie Hill",2
"East Harlem",1
Inwood,1
"North Sutton Area",1
`

#### Which neighborhoods have the most Italian restaurants? 

```SQL
select nodes.neighborhood, count(*) as count
from nodes, nodes_tags
where nodes.id = nodes_tags.id
and nodes.neighborhood != ''
and nodes_tags.key = 'cuisine'
and nodes_tags.value = 'italian'
group by nodes.neighborhood
order by count desc
limit 10;
```

`
"Upper West Side",24
Chelsea,14
Midtown,14
"Greenwich Village",8
Clinton,7
"East Village",6
Gramercy,6
"Financial District",5
"West Village",5
"Upper East Side",4
`

Obviously, this type of query can be used for any type of restaurant by simply adjusting the condition based on the value of the cuisine tag. 

Given that there is not a consistent number of restaurants across neighborhoods (and also not necessarily consistently thorough data), I thought it would be more interesting to look at the ratio of locations of a particular type of restaurant to the total number of restaurants in that neighborhood. In theory, this should show us which neighborhoods are particularly known for a certain type of cuisine. 

Because of my extensive knowledge of the New York food scene (at least I like to think it is extensive!), I had some hypotheses going into this study. Specifically:
* Indian food would be most represented in Gramercy, due to the stretch of Lexington Avenue known as [Curry Hill](http://www.grubstreet.com/bestofnewyork/best-indian-restaurant-curry-hill-nyc.html). (note: this area is traditionally considered part of the neighborhood of Murray Hill, but falls within the boundaries of Gramercy in the neighborhood data we are using here.)
* Chinese food would be most represented in Chinatown (duh) and the Lower East Side (which is adjacent to Chinatown)
* Korean food would be most represented in the Garment District, as that is where Koreatown falls (once again, in this neighborhood data). 
* French food would be more represented in the more upscale neighborhoods, as it is traditionally a more affluent cuisine
* Steakhouses would be more represented in midtown and far downtown neighborhoods, as these are the most office-heavy neighborhoods

Below is the query I used, with the nodes_tags.value condition adjusted based on the cuisine.

```SQL
select cuisinecount.neighborhood, round((cuisinecount.count * 1.0) / (restaurantcount.count * 1.0), 2) as cuisineratio
from (select nodes.neighborhood, count(*) as count from nodes, nodes_tags where nodes.id = nodes_tags.id and nodes_tags.key = 'cuisine' and nodes_tags.value = 'steak' group by nodes.neighborhood) as cuisinecount,  
(select nodes.neighborhood, count(*) as count from nodes, nodes_tags where nodes.id = nodes_tags.id and nodes_tags.key = 'cuisine' group by nodes.neighborhood) as restaurantcount
where cuisinecount.neighborhood = restaurantcount.neighborhood
and cuisinecount.neighborhood != ''
order by cuisineratio desc
limit 10;
```

##### Indian
`
Gramercy,0.12
Chelsea,0.05
"Financial District",0.05
"Lower East Side",0.05
"Upper West Side",0.05
"Garment District",0.04
Midtown,0.03
Clinton,0.02
"Greenwich Village",0.02
"Upper East Side",0.02
`

Just as anticipated! 

##### Chinese
`
Chinatown,0.86
"Battery Park",0.2
Yorkville,0.2
"Lower East Side",0.17
"Washington Heights",0.17
"Morningside Heights",0.06
"Upper West Side",0.06
"Greenwich Village",0.05
Harlem,0.05
Midtown,0.04
`

Unsurprisingly, Chinatown is nearly 100% Chinese restaurants. The Lower East Side is also near the top.

##### Korean
`
"Garment District",0.19
Chelsea,0.03
"Financial District",0.03
Clinton,0.02
Gramercy,0.02
Midtown,0.01
`

Nearly all of the Korean restaurants in the dataset fall in the Garment District.

##### French
`
"North Sutton Area",1.0
Soho,0.26
Yorkville,0.2
"West Village",0.08
Gramercy,0.06
"Morningside Heights",0.06
Harlem,0.05
Midtown,0.04
"Murray Hill",0.04
Tribeca,0.04
`

The top results here are in fact among the cities more affluent neighborhoods. [Sutton](https://en.wikipedia.org/wiki/York_Avenue_/_Sutton_Place) and [Yorkville](https://en.wikipedia.org/wiki/Yorkville,_Manhattan) are both part of the Upper East Side, while [Soho](https://en.wikipedia.org/wiki/SoHo,_Manhattan) and the [West Village](https://en.wikipedia.org/wiki/West_Village) are perhaps the two Manhattan neighborhoods most often cited as examples of gentrification. 

##### Steak
`
Clinton,0.07
"Garment District",0.04
Midtown,0.04
Tribeca,0.04
"Financial District",0.03
"East Village",0.01
Gramercy,0.01
`

These results basically fall in line with expectations - Clinton, Garment District, and Midtown are all in Midtown Manhattan, and Tribeca and the Financial District are at the southern end of the island.

### Other areas of interest
Moving beyond restaurants, below are some other queries I used to explore the data.

#### What neighborhood has the most bars? 

```SQL
select nodes.neighborhood, count(*) as count
from nodes, nodes_tags
where nodes.id = nodes_tags.id
and nodes_tags.key = 'amenity'
and nodes_tags.value = 'bar'
and nodes.neighborhood != ''
group by nodes.neighborhood
order by count desc
limit 10; 
```

`
Chelsea,25
"East Village",22
"Greenwich Village",19
"Lower East Side",18
Midtown,18
"West Village",10
Clinton,7
Gramercy,7
"Upper West Side",7
"Garment District",5
`

The top results here - particularly East Village and the Lower East Side - are among the most popular destinations for nightlife in New York, and also (somewhat relatedly) have a higher concentration of young tenants. 

#### How many locations associated with each religion? 

```SQL
select nodes_tags.value, count(*) as count
from nodes_tags
where nodes_tags.key = 'religion'
group by nodes_tags.value
order by count desc;
```

`
christian,242
jewish,22
buddhist,7
muslim,5
hindu,3
`

#### Is there one post office per zip code? 
It is generally true that each zip code has (at least) one post office associated with it. While this is not always the case, and it is possible that the post office that serves a certain zip code might not fall geographically within that zip code, I thought it would be interesting to see if the data bore this out. 

```SQL
select nodescomb.value, count(*) as count
from nodes_tags, (select nodes_tags.id, nodes_tags.value from nodes, nodes_tags where nodes.id = nodes_tags.id and
nodes_tags.key = 'postcode') as nodescomb
where nodes_tags.id = nodescomb.id
and nodes_tags.key = 'amenity'
and nodes_tags.value = 'post_office'
group by nodescomb.value
order by count desc;
```

`
10023,1
10037,1
11201,1
11222,1
`

Needless to say, that was not the result I was hoping for or expecting. As a sanity check, I ran the same query with restaurants instead, and obtained a better result:

`
10025,54
10011,50
10003,49
11211,48
10019,47
10024,34
10014,28
10036,27
10010,24
10001,23
`

My theory is that the query is correct, but there simply isn't enough zip code/post office data to get a meaningful result when searching for nodes that have both of those data points.

#### Where are the gas stations? 
There are so few gas stations in Manhattan! Which neighborhoods are they found in? 

```SQL
select nodes.neighborhood, count(*) as count
from nodes, nodes_tags
where nodes.id = nodes_tags.id
and nodes_tags.key = 'amenity'
and nodes_tags.value = 'fuel'
group by nodes.neighborhood
order by count desc; 
```

`
"",40
Harlem,3
Clinton,2
Inwood,2
Chelsea,1
"Morningside Heights",1
"Upper East Side",1
"Upper West Side",1
"Washington Heights",1
"West Village",1
`

This makes total sense - it's probably mostly in Jersey! Outside of my range. Let's investigate.

```SQL
select nodescomb.value, count(*) as count
from nodes_tags, (select nodes_tags.id, nodes_tags.value from nodes, nodes_tags where nodes.id = nodes_tags.id and
nodes_tags.key = 'city') as nodescomb
where nodes_tags.id = nodescomb.id
and nodes_tags.key = 'amenity'
and nodes_tags.value = 'fuel'
group by nodescomb.value
order by count desc;
```

`
Astoria,1
"New York",1
`

That didn't show me anything... Let's take a look at all of the tag data for the nodes that have fuel. 

```SQL
select nodes_tags.id, nodes_tags.key, nodes_tags.value
from nodes_tags, (select nodes_tags.id, nodes_tags.value from nodes, nodes_tags where nodes.id = nodes_tags.id and
nodes_tags.key = 'amenity' and nodes_tags.value = 'fuel') as fuelnodes
where nodes_tags.id = fuelnodes.id;
```

`
617741648,amenity,fuel
617741648,tourism,viewpoint
617741652,amenity,fuel
1549441481,amenity,fuel
1549441481,housenumber,"Gas Station"
1549441481,name,Getty
1549441481,shop,yes
1549441481,street,Broadway
1760541821,amenity,fuel
1760541821,brand,Exxon
1825848826,amenity,fuel
1825848826,brand,Mobil
1825848826,shop,no
1873813946,amenity,fuel
1873813946,brand,Sunoco
1873813946,name,Sunoco
1955926332,amenity,fuel
1955926332,source,"local knowledge"
2031099086,amenity,fuel
2065151595,amenity,fuel
2065151595,car_wash,yes
2065151595,city,"New York"
2065151595,housenumber,3620
2065151595,name,Citgo
2065151595,street,"Queens Boulevard"
2065156021,amenity,fuel
2065156021,name,Hess
2070041661,amenity,fuel
2070041661,brand,BP
2070041661,car_wash,no
2070041661,city,Astoria
2070041661,housenumber,40-08
2070041661,name,BP
2070041661,street,"30th Avenue"
2070983542,amenity,fuel
2070983542,brand,Getty
2070984278,amenity,fuel
2070984278,brand,Getty
2070984287,amenity,fuel
2070984287,name,Sunoco
2070984287,opening_hours,24/7
2270766963,amenity,fuel
2270766963,brand,Delta
2270766963,name,Delta
2270766963,operator,Delta
2270766965,amenity,fuel
2270766965,brand,Sunoco
2270766965,name,Sunoco
2270766965,operator,Sunoco
2349831026,amenity,fuel
2349831028,amenity,fuel
2562438645,amenity,fuel
2562438645,name,Shell
2562438645,shop,kiosk
2592264892,amenity,fuel
2592264892,name,Mobil
2592317399,amenity,fuel
2592317399,name,Sunoco
2592331353,amenity,fuel
2592331353,name,Sunoco
2629371845,amenity,fuel
2629371845,brand,Shell
2629371845,name,Shell
2803634646,amenity,fuel
2803634646,name,BP
2803634646,operator,BP
2803634648,amenity,fuel
2838087246,amenity,fuel
2838087246,brand,BP
2883793331,amenity,fuel
2883793331,name,BP
2902109923,amenity,fuel
2902109923,brand,Mobil
3026945576,amenity,fuel
3026945576,brand,BP
3132247191,amenity,fuel
3132247191,name,BP
3132247192,amenity,fuel
3132247192,name,Gulf
3132259234,amenity,fuel
3132259234,name,Exxon
3132260084,amenity,fuel
3132260084,name,BP
3132266255,amenity,fuel
3132266255,diesel,yes
3132266255,name,Exxon
3132266256,amenity,fuel
3132266256,name,"Fuel 4"
3132266660,amenity,fuel
3132266660,name,Sunoco
3132267208,amenity,fuel
3132267208,name,Mobil
3163917381,amenity,fuel
3163917381,source,Bing
3316229665,amenity,fuel
3569402156,amenity,fuel
3569402156,brand,CITGO
3579460193,amenity,fuel
3579460193,name,Mobile
3678184325,amenity,fuel
3678184325,description,"Reifendervice f√ºr wheelchair"
3678184325,name,Reifenservice
3678184325,wheelchair,limited
3769312757,amenity,fuel
3785177672,amenity,fuel
3785177672,name,Mobil
3798081329,amenity,fuel
3810178785,amenity,fuel
3810178785,brand,Gulf
3810178785,name,"Gas Station"
3863002358,amenity,fuel
3863002358,brand,Sanoco
3863002358,car_wash,no
3863002358,name,Sunoco
3865095759,amenity,fuel
3865095759,name,Gulf
3865096257,amenity,fuel
3865096257,name,Shell
3867720991,amenity,fuel
3867720991,name,Mobile
4014980158,amenity,fuel
4014980158,brand,Speedway
4111168788,amenity,fuel
4111168788,name,Mobil
4111168989,amenity,fuel
4111168989,name,Speedway
`

This shows exaclty what I feared - most of the data *really* leaves something to be desired in terms of addresses! Almost all of the nodes are simply amenity/name pairs, with the occasional partial address. I don't understand why that is.

## Improving the Data
There are a few main issues with the data that limit this analysis.

First, as exemplified by the post office and gas station queries above, the address data is frustratingly incomplete. One potential solution to this is using [reverse geocoding](https://en.wikipedia.org/wiki/Reverse_geocoding) for all nodes. All nodes *must* have latitude and longitude coordinates, and thus we could use reverse geocoding to automatically fill in, at a minimum, city/state/country and postal code data for any node that is created (assigning street names/building addresses might be a bit too aggressive). Considering how many nodes are lacking this basic information, I feel this would strongly enhance the dataset. The main limitation to this solution is that it might be difficult to find a reverse geocoding service to utilize. Also, any time a programmatic solution like this is introduced into the service, it leaves open the possibility that a huge amount of bad data is generated and put onto the map if there is a bug in the reverse geocoding. One way to avoid this would be to ask users to sign off on the data before it is submitted, to ensure it looks sensible. That should work quite well.

On a related note, the data seems to be pretty incomplete in general. I took a quick look around my neighborhood on the OpenStreetMap website, and there are a bunch of amenities missing! Obviously this is a much tougher problem to solve than the missing address data... As a start, there are multiple sources of restaurant/bar/etc. data that other applications make use of - Foursquare and Yelp are the two most obvious examples. Perhaps this goes against the tenets of OpenStreetMap? I'm not sure if they would be allowed to pay for Foursquare/Yelp's data and then make it freely available to everyone (likely not?). If pulling this data from another service isn't an option, than the only solution is straightforward - get more users! Unfortunately the means of accomplishing this are not nearly as straighforward. 

A final issue, specific to my analysis, is that the neighborhood data I used is, as I mentioned earlier, a bit outdated. Aside from scouring the web for another source of freely-available data, the only solution I can think of here is to simply go about creating the data myself! This is doable, though obviously time-intensive. 



## Conclusions
Despite the aforementioned limitations to this dataset, I am quite pleased with the analysis I was able to perform, specifically on the restaurant/cuisine data points. The neighborhood data from Zillow integrated easily with the OpenStreetMap data. It's clear that different neighborhoods have different "flavors" of cuisine associated with them, and these mostly match up with conventional wisdom. With a combination of more complete amenity and address data and more accurate neighborhood data, the number of insights we could discover would be even greater.

## References 
http://effbot.org/zone/celementtree.htm  
http://www.zillow.com/howto/api/neighborhood-boundaries.htm  
https://pypi.python.org/pypi/pyshp  
https://en.wikipedia.org/wiki/Shapefile  
http://gis.stackexchange.com/questions/121469/get-shape-file-polygon-attribute-value-at-a-specific-point-using-python-e-g-vi  
https://mapzen.com/data/metro-extracts/  


## Appendix

### Core functions


```python
#fix street types
import re
street_type_re = re.compile(r'\b\S+\.?$', re.IGNORECASE) #recognize street types

#list of expected street types
expected_streets = ["Street", "Avenue", "Boulevard", "Drive", "Court", "Place", "Square", "Lane", "Road", 
            "Trail", "Parkway", "Commons", 'Turnpike', 'Plaza', 'South', 'North', 'A', 'B', 'C', 'D',
                   'Broadway', 'Expressway', 'Terrace', 'Way', 'Concourse', 'Heights', 'Alley',
                   'Extension', 'East', 'West', 'Bowery', 'Terminal', 'Slip', 'Americas', 'Walk', 'Row',
                   'Circle', 'Hill', 'Crescent', 'Bush', 'Center', 'Piers', 'Walk', 'Loop', 'Highway', 'Rico',
                   'Oval', 'Finest', 'Island', 'Village', 'Mews', 'Park']

#this handles the case of finding a highway, e.g. 'Route 287' - we won't want to take any action on this
highways = ['Route', 'Highway']

#map to expected street types
street_mapping = { "St": "Street",
            "St.": "Street",
            "Ave": "Avenue",
            "Rd.": "Road",
           'Rd': 'Road',
           'st': 'Street',
           'Pky': 'Parkway',
           'avenue': 'Avenue',
           'street': 'Street',
           'Ct': 'Court',
           'ave': 'Avenue',
           'Avene': 'Avenue',
           'Ave.': 'Avenue',
           'Dr.': 'Drive',
           'Plz': 'Plaza',
           'Steet': 'Street',
           'drive': 'Drive',
           'road': 'Road',
           'Ave,': 'Ave',
           'Blvd': 'Boulevard',
           'Pkwy': 'Parkway',
           'ST': 'Street',
           'Broadway.': 'Broadway'
            }

def fix_street(street, mapping):
    if street.split()[-1] not in mapping: #if not something we're fixing
        return None
    else:
        x = re.search(street_type_re, street).start() #find where street name starts
        if street.split()[0] in highways: #don't want to correct 'Route 287' for example. 
            return street
        else:
            return street[:x] + mapping[street[x:]]
```


```python
#fix street prefixes
prefix_map = {'N': 'North',
              'N.': 'North',
              'S': 'South',
              'S.': 'South',
              'W': 'West',
              'W.': 'West',
              'E': 'East',
              'E.': 'East',
              'Rt': 'Route',
              'Rt.': 'Route',
              'Hwy': 'Highway',
              'Hwy.': 'Highway'}

def fix_streetprefix(street, mapping):
    prefix = street.split()[0] #get first word in street name
    if prefix in prefix_map: #adjust if necessary
        return street.replace(prefix, prefix_map[prefix], 1)
    return street
```


```python
#postal code fixer function
def fix_postcode(postcode):
    if postcode.isdigit() and len(postcode) == 5: #valid postal code
        return postcode
    elif len(postcode) < 5: #definitely invalid if fewer than 5 characters
        return None
    elif postcode[3:].isdigit() and len(postcode[3:]) == 5: #fixes, for example, 'NJ 07036'
        return postcode[3:]
    elif postcode[:5].isdigit() and postcode[5] == '-': #fixes, for example, '08901-1340'
        return postcode[:5]
    elif postcode.strip().isdigit() and len(postcode.strip()) == 5: #fixes, for example, ' 10010'
        return postcode.strip()
    else: #if still not fixed, skip this tag
        return None
    
```


```python
def fix_city(city):
    city = city.title() #correct capitalization scheme
    city = city.strip() #strip whitespace
    if city == 'New York City': #'New York City' should be 'New York'
        return 'New York'
    elif re.search(',', city): #capture 'New York, NY' for example
        a = city[:re.search(',', city).start()]
        if a == 'New York City':
            return 'New York'
        return a
    splits = city.split() #capture 'Queens NY' for example
    if splits[-1].upper() in ['NY', 'NJ', 'CT']:
        a = city[:-2].strip()
        if a == 'New York City':
            return 'New York'
        return a
    return city #just leave it as is if none of these issues are captured
```


```python
expected_cuisine = ['nepalese', 'mexican', 'chinese', 'german', 'japanese', 'russian', 'wine bar', 'asian', 
                    'brazilian', 'coffee shop', 'burger', 'vietnamese', 'ramen', 'sandwich', 'american', 'irish',
                    'venezuelan', 'french', 'vegan', 'indian', 'spanish', 'italian', 'indonesian', 'serbian', 
                    'austrian', 'steak', 'caribbean', 'mediterranean', 'barbecue', 'falafel', 'colombian', 
                    'cuban', 'scandinavian', 'oysters', 'korean', 'diner', 'filipino', 'latin american', 
                    'vegetarian', 'turkish', 'ethiopian', 'kosher', 'romanian', 'jamaican', 'southern', 
                    'peruvian', 'belgian', 'taiwanese', 'nordic', 'crepe', 'thai', 'scottish', 'lebanese', 
                    'pakistani', 'ecuadorian', 'donut', 'israeli', 'swiss', 'ice cream', 'tibetan', 'australian',
                    'greek', 'seafood', 'pizza', 'english', 'cambodian', 'dominican', 'chicken']
```


```python
cuisine_mapping = {'brasilian': 'brazilian', 'doughnut': 'donut', 'tacos': 'mexican', 'fish': 'seafood',
                  'steakhouse': 'steak', 'steak house': 'steak', 'subs': 'sandwich', 'sandwiches': 'sandwich',
                  'tapas': 'spanish', 'sushi': 'japanese', 'taco': 'mexican', 'basque': 'spanish'}
```


```python
#clean up cuisine data
def fix_cuisine(cuisine):
    cuisine = cuisine.lower() #convert to lower case
    if cuisine in expected_cuisine:
        return cuisine
    if cuisine in cuisine_mapping:
        return cuisine_mapping[cuisine]
    if cuisine.find(';'): #for lists with semicolon
        words = cuisine.split(';')
        for word in words:
            clean = clean_cuisine(word)
            if clean:
                return clean
    if cuisine.find(','): #for lists with commas
        words = cuisine.split(',')
        for word in words:
            clean = clean_cuisine(word)
            if clean:
                return clean
    if cuisine.find('/'): #for lists with slashes
        words = cuisine.split('/')
        for word in words:
            clean = clean_cuisine(word)
            if clean:
                return clean
    if cuisine.find('-'): #for cuisines split with a hyphen
        words = cuisine.split('-')
        for word in words:
            clean = clean_cuisine(word)
            if clean:
                return clean            
    if cuisine.find('_'): #if separated with _'s
        cuisine = cuisine.replace('_', ' ')
        if cuisine in expected_cuisine:
            return cuisine
        words = cuisine.split()
        for word in words:
            clean = clean_cuisine(word)
            if clean:
                return clean 
    return None
```


```python
#helper function
def clean_cuisine(word):
    word = word.replace('_', ' ') #change _'s to spaces
    word = word.strip()
    if word in expected_cuisine:
        return word
    if word[:-1] in expected_cuisine: #if simple plural (e.g. 'donuts')
        return word[:-1]
    if word in cuisine_mapping:
        return cuisine_mapping[word]
    if word[:-1] in cuisine_mapping:
        return cuisine_mapping[word[:-1]]
    return None
```


```python
#function that tests if a point is inside a polygon; 
#source: http://gis.stackexchange.com/questions/121469/get-shape-file-polygon-attribute-value-at-a-specific-point-using-python-e-g-vi
def point_in_poly(x,y,poly):

   # check if point is a vertex
   if (x,y) in poly: return False

   # check if point is on a boundary
   for i in range(len(poly)):
      p1 = None
      p2 = None
      if i==0:
         p1 = poly[0]
         p2 = poly[1]
      else:
         p1 = poly[i-1]
         p2 = poly[i]
      if p1[1] == p2[1] and p1[1] == y and x > min(p1[0], p2[0]) and x < max(p1[0], p2[0]):
         return True

   n = len(poly)
   inside = False

   p1x,p1y = poly[0]
   for i in range(n+1):
      p2x,p2y = poly[i % n]
      if y > min(p1y,p2y):
         if y <= max(p1y,p2y):
            if x <= max(p1x,p2x):
               if p1y != p2y:
                  xints = (y-p1y)*(p2x-p1x)/(p2y-p1y)+p1x
               if p1x == p2x or x <= xints:
                  inside = not inside
      p1x,p1y = p2x,p2y

   if inside: return True
   else: return False
```


```python
#creates list of name-points tuples
neighborhoods = [] 
for entry in sf.iterShapeRecords():
    if entry.record[1] == 'New York': #if county is New York (this is the island of Manhattan)
        neighborhoods.append((entry.record[3], entry.shape.points))
```


```python
#assigns neighborhood, given x and y coordinates.
def assign_neighborhood(x, y, neighborhoods):
    for neighborhood in neighborhoods:
        if point_in_poly(float(x),float(y),neighborhood[1]):
            return neighborhood[0]
    return None
```


```python
import shapefile
sf = shapefile.Reader('/Users/chadhorner/Downloads/ZillowNeighborhoods-NY/ZillowNeighborhoods-NY')
```


```python
import csv
import codecs
import re
import xml.etree.cElementTree as ET

import cerberus
import schema

#file names for csvs to create
NODES_PATH = "nodes.csv"
NODE_TAGS_PATH = "nodes_tags.csv"
WAYS_PATH = "ways.csv"
WAY_NODES_PATH = "ways_nodes.csv"
WAY_TAGS_PATH = "ways_tags.csv"

#REs to find problem tags or tags with colons (e.g. 'addr:postcode')
LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')
PROBLEMCHARS = re.compile(r'[=\+/&<>;\'"\?%#$@\,\. \t\r\n]')

SCHEMA = schema.schema

# Make sure the fields order in the csvs matches the column order in the sql table schema
NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp', 'neighborhood']
NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']
WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']
WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']
WAY_NODES_FIELDS = ['id', 'node_id', 'position']

#shape element function. handles the cases outlined above - different address fields as well as cuisine
def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,
                  problem_chars=PROBLEMCHARS, neighborhoods = neighborhoods):

    node_attribs = {}
    way_attribs = {}
    way_nodes = []
    tags = []  # Handle secondary tags the same way for both node and way elements

    #do tags first; same for either node or way
    for tag in element.findall('tag'):
        tagdict = {}
        tagdict['id'] = element.attrib['id']
        k = tag.attrib['k']
        if re.search(problem_chars, k): #skip if problem characters
            continue
        elif k == 'addr:postcode': #if postcode
            postcode = tag.attrib['v']
            tagdict['type'] = 'addr'
            tagdict['key'] = 'postcode'
            if (postcode.isdigit() and len(postcode) == 5): #if this is valid postcode
                tagdict['value'] = postcode
            else:
                postcode = fix_postcode(postcode) #else try to fix
                if postcode: #if fixed, use it
                    tagdict['value'] = postcode
                else: #if not, skip this tag
                    continue
        elif k == 'addr:street': #if street
            street = tag.attrib['v']
            tagdict['type'] = 'addr'
            tagdict['key'] = 'street'
            street = fix_streetprefix(street, prefix_map) #fix prefix, if necessary            
            m = street_type_re.search(street) #find where street starts
            if m:
                street_type = m.group()
                if street_type in expected_streets: #if this is a good street
                    tagdict['value'] = street
                if street_type not in expected_streets: #if not, try to fix
                    street = fix_street(street, street_mapping)
                    if street:
                        tagdict['value'] = street
                    else:
                        continue
            else: #if this doesn't contain a street name at all
                continue
        elif k == 'addr:city': #if city
            tagdict['value'] = fix_city(tag.attrib['v'])
            tagdict['type'] = 'addr'
            tagdict['key'] = 'city'
        elif k == 'cuisine': #if cuisine
            cuisine = tag.attrib['v']
            tagdict['type'] = 'regular'
            tagdict['key'] = 'cuisine'
            if cuisine in expected_cuisine: #if expected, we're good
                tagdict['value'] = cuisine
            else: #else fix it
                cuisine = fix_cuisine(cuisine)
                if cuisine:
                    tagdict['value'] = cuisine
                else: #skip it if it's bad
                    continue                
        elif re.search(LOWER_COLON, k): #if colon in k
            start = re.search(':', k).start()
            tagdict['type'] = k[:start]
            tagdict['key'] = k[start+1:]
            tagdict['value'] = tag.attrib['v']
        else: #else it is a normal tag
            tagdict['type'] = 'regular'
            tagdict['key'] = k
            tagdict['value'] = tag.attrib['v']
        tags.append(tagdict)

    if element.tag == 'node':
        for field in node_attr_fields[:-1]:
            node_attribs[field] = element.attrib[field]
        neighborhood = assign_neighborhood(float(element.attrib['lon']), float(element.attrib['lat']), neighborhoods)
        if neighborhood: #if in a neighborhood, assign
            node_attribs['neighborhood'] = neighborhood
        return {'node': node_attribs, 'node_tags': tags}
    elif element.tag == 'way':
        for field in way_attr_fields:
            way_attribs[field] = element.attrib[field]
        count = 0 #initialize counter
        for tag in element.findall('nd'): #look through nd tags
            tagdict = {}
            tagdict['id'] = element.attrib['id']
            tagdict['node_id'] = tag.attrib['ref']
            tagdict['position'] = count
            count += 1
            way_nodes.append(tagdict)            
        return {'way': way_attribs, 'way_tags': tags, 'way_nodes': way_nodes}
```


```python
#this code is the same as the code used in the exercises in class
def process_map(file_in, validate):
    """Iteratively process each XML element and write to csv(s)"""

    with codecs.open(NODES_PATH, 'w') as nodes_file, \
         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \
         codecs.open(WAYS_PATH, 'w') as ways_file, \
         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \
         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:

        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)
        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)
        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)
        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)
        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)

        nodes_writer.writeheader()
        node_tags_writer.writeheader()
        ways_writer.writeheader()
        way_nodes_writer.writeheader()
        way_tags_writer.writeheader()

        validator = cerberus.Validator()

        for element in get_element(file_in, tags=('node', 'way')):
            el = shape_element(element)
            if el:
                if validate is True:
                    validate_element(el, validator)

                if element.tag == 'node':
                    nodes_writer.writerow(el['node'])
                    node_tags_writer.writerows(el['node_tags'])
                elif element.tag == 'way':
                    ways_writer.writerow(el['way'])
                    way_nodes_writer.writerows(el['way_nodes'])
                    way_tags_writer.writerows(el['way_tags'])

def get_element(osm_file, tags=('node', 'way', 'relation')):
    """Yield element if it is the right type of tag"""

    context = ET.iterparse(osm_file, events=('start', 'end'))
    _, root = next(context)
    for event, elem in context:
        if event == 'end' and elem.tag in tags:
            yield elem
            root.clear()


def validate_element(element, validator, schema=SCHEMA):
    """Raise ValidationError if element does not match schema"""
    if validator.validate(element, schema) is not True:
        field, errors = next(validator.errors.iteritems())
        message_string = "\nElement of type '{0}' has the following errors:\n{1}"
        error_string = pprint.pformat(errors)
        
        raise Exception(message_string.format(field, error_string))


class UnicodeDictWriter(csv.DictWriter, object):
    """Extend csv.DictWriter to handle Unicode input"""

    def writerow(self, row):
        super(UnicodeDictWriter, self).writerow({
            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()
        })

    def writerows(self, rows):
        for row in rows:
            self.writerow(row)
```

### schema.py


```python
# schema.py

# coding: utf-8

# In[1]:

# Note: The schema is stored in a .py file in order to take advantage of the
# int() and float() type coercion functions. Otherwise it could easily stored as
# as JSON or another serialized format.

schema = {
    'node': {
        'type': 'dict',
        'schema': {
            'id': {'required': True, 'type': 'integer', 'coerce': int},
            'lat': {'required': True, 'type': 'float', 'coerce': float},
            'lon': {'required': True, 'type': 'float', 'coerce': float},
            'user': {'required': True, 'type': 'string'},
            'uid': {'required': True, 'type': 'integer', 'coerce': int},
            'version': {'required': True, 'type': 'string'},
            'changeset': {'required': True, 'type': 'integer', 'coerce': int},
            'timestamp': {'required': True, 'type': 'string'},
            'neighborhood': {'required': False, 'type': 'string'}
        }
    },
    'node_tags': {
        'type': 'list',
        'schema': {
            'type': 'dict',
            'schema': {
                'id': {'required': True, 'type': 'integer', 'coerce': int},
                'key': {'required': True, 'type': 'string'},
                'value': {'required': True, 'type': 'string'},
                'type': {'required': True, 'type': 'string'}
            }
        }
    },
    'way': {
        'type': 'dict',
        'schema': {
            'id': {'required': True, 'type': 'integer', 'coerce': int},
            'user': {'required': True, 'type': 'string'},
            'uid': {'required': True, 'type': 'integer', 'coerce': int},
            'version': {'required': True, 'type': 'string'},
            'changeset': {'required': True, 'type': 'integer', 'coerce': int},
            'timestamp': {'required': True, 'type': 'string'}
        }
    },
    'way_nodes': {
        'type': 'list',
        'schema': {
            'type': 'dict',
            'schema': {
                'id': {'required': True, 'type': 'integer', 'coerce': int},
                'node_id': {'required': True, 'type': 'integer', 'coerce': int},
                'position': {'required': True, 'type': 'integer', 'coerce': int}
            }
        }
    },
    'way_tags': {
        'type': 'list',
        'schema': {
            'type': 'dict',
            'schema': {
                'id': {'required': True, 'type': 'integer', 'coerce': int},
                'key': {'required': True, 'type': 'string'},
                'value': {'required': True, 'type': 'string'},
                'type': {'required': True, 'type': 'string'}
            }
        }
    }
}



```

### Various testing functions


```python
from collections import defaultdict
```


```python
#this code creates the sample file; it is not really necessary for the actual project
import os
import xml.etree.cElementTree as ET  # Use cElementTree or lxml if too slow

OSM_FILE = "newyork.osm"  # Replace this with your osm file
SAMPLE_FILE = "sample.osm"

k = 500 # Parameter: take every k-th top level element

def get_element(osm_file, tags=('node', 'way', 'relation')):
    """Yield element if it is the right type of tag

    Reference:
    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python
    """
    context = iter(ET.iterparse(osm_file, events=('start', 'end')))
    _, root = next(context)
    for event, elem in context:
        if event == 'end' and elem.tag in tags:
            yield elem
            root.clear()


with open(SAMPLE_FILE, 'wb') as output:
    output.write('<?xml version="1.0" encoding="UTF-8"?>\n')
    output.write('<osm>\n  ')

    # Write every kth top level element
    for i, element in enumerate(get_element(OSM_FILE)):
        if i % k == 0:
            output.write(ET.tostring(element, encoding='utf-8'))
        if i > 3000000000:
            break

    output.write('</osm>')
```


```python
#test post code fixer
count = 1
START = 0
END = 10000000
for event, elem in ET.iterparse(OSM_FILE):
    if count >= START:
        if (elem.tag == 'node' or elem.tag == 'way'):
            for tag in elem.iter('tag'):
                if tag.attrib['k'] == 'addr:postcode':
                    postcode = tag.attrib['v']
                    if not (postcode.isdigit() and len(postcode) == 5):
                        print postcode + " --> " + fix_postcode(postcode)
    count += 1
    if count > END:
        break
```


```python
#test street fixer
count = 1
START = 0
END = 50000000

unexpecteds = set()

for event, elem in ET.iterparse(OSM_FILE):
    if count >= START:
        if (elem.tag == 'node' or elem.tag == 'way'):
            for tag in elem.iter('tag'):
                if tag.attrib['k'] == 'addr:street':
                    street = tag.attrib['v']
                    m = street_type_re.search(street)
                    if m:
                        street_type = m.group()
                        if street_type not in expected_streets:
                            print street + " --> " + fix_street(street, mapping)
                            unexpecteds.add(street)
    count += 1
    if count > END:
        break
```


```python
#this was me testing to see which restaurants would get assigned a value
isrestaurant = False
restaurantcount = {}
cuisinecount = defaultdict(int)
for neighborhood in neighborhoods:
    restaurantcount[neighborhood[0]] = 0
for event, elem in ET.iterparse(OSM_FILE):
    if elem.tag == 'node':
        for tag in elem.iter('tag'):
            if tag.attrib['k'] == 'name':
                name = tag.attrib['v']
            if tag.attrib['k'] == 'amenity':
                if tag.attrib['v'] == 'restaurant':
                    isrestaurant = True
                    neighborhood = assign_neighborhood(float(elem.attrib['lon']), float(elem.attrib['lat']), neighborhoods)
            if tag.attrib['k'] == 'cuisine':
                cuisinecount[tag.attrib['v']] += 1
        if isrestaurant and neighborhood:
            print name + ' is in the neighborhood of ' + neighborhood
            restaurantcount[neighborhood] += 1
    isrestaurant = False
```


```python

```
